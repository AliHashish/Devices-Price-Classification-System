{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import load, dump\n",
    "\n",
    "\n",
    "\n",
    "dataset = \"train\"\n",
    "df = pd.read_excel('Data/Clean_'+dataset+'.xlsx')       # This is the preprocessed data\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen features: Index(['ram', 'battery_power', 'px_width', 'px_height'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "correlation_with_val_eur = np.abs(df.corrwith(df['price_range']))            # absolute to handle -ve values\n",
    "sorted_correlations = correlation_with_val_eur.sort_values(ascending=False)\n",
    "print(\"Chosen features:\", sorted_correlations[1:5].index)                    # Choosing the top 4 features (excluding the target column)\n",
    "# We can't rely on the ram only, as that would make the model prone to overfitting, and easily affected by noise\n",
    "\n",
    "x_data = df[sorted_correlations[1:5].index]             \n",
    "y_data = df['price_range']\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelEvaluation(golden_output, predicted_output):\n",
    "    print(\"Accuracy: \", accuracy_score(golden_output, predicted_output))\n",
    "    # Accuracy is a good measure as the price_range is balanced\n",
    "\n",
    "    print(\"Classification Report: \\n\", classification_report(golden_output, predicted_output))\n",
    "    # We will also look at the precision and recall, as well as f1-score to see if the model is biased towards a particular class\n",
    "\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(golden_output, predicted_output))\n",
    "    # Finally, we will check the confusion matrix to get even finer insights\n",
    "    \n",
    "    print(\"======================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be working with a small number of features, KNN should theoretically be a good estimator. It would converge quickly due to the low dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "KNN\n",
      "Accuracy:  0.9047619047619048\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       106\n",
      "           1       0.88      0.87      0.87        89\n",
      "           2       0.88      0.85      0.86       107\n",
      "           3       0.92      0.93      0.92        97\n",
      "\n",
      "    accuracy                           0.90       399\n",
      "   macro avg       0.90      0.90      0.90       399\n",
      "weighted avg       0.90      0.90      0.90       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[103   3   0   0]\n",
      " [  6  77   6   0]\n",
      " [  0   8  91   8]\n",
      " [  0   0   7  90]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "print(\"======================================================\\nKNN\")\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(x_train, y_train)\n",
    "y_pred = knn.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# KNN shows promising results for the first experiment, however it's not very capable\n",
    "# of predicting the 1 and 2 price ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN shows promising results for the first experiment, however it's not very capable of predicting the 1 and 2 price ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is rather simple and the price_range is heavily dependent on the ram, as well as the rest of the features. It can be predicted using a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Logistic Regression\n",
      "Accuracy:  0.9423558897243107\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       106\n",
      "           1       0.91      0.96      0.93        89\n",
      "           2       0.95      0.85      0.90       107\n",
      "           3       0.92      0.97      0.94        97\n",
      "\n",
      "    accuracy                           0.94       399\n",
      "   macro avg       0.94      0.94      0.94       399\n",
      "weighted avg       0.94      0.94      0.94       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[106   0   0   0]\n",
      " [  2  85   2   0]\n",
      " [  0   8  91   8]\n",
      " [  0   0   3  94]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "print(\"======================================================\\nLogistic Regression\")\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# As expected, the logistic model does indeed fare well in this case, as the data is\n",
    "# linearly separable to some extent. It shows slightly better results than KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the logistic model does indeed fare well in this case, as the data is linearly separable to some extent. It shows slighlty superior results to the KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are prone to overfitting, however, random forests are a good way to mitigate this issue. Since the data is not very complex, some rules can be deduced such as the ones proposed in the EDA analysis. As such, a random forest model should be able to predict the price range with a reasonably good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Random Forest\n",
      "Accuracy:  0.9323308270676691\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       106\n",
      "           1       0.88      0.91      0.90        89\n",
      "           2       0.95      0.90      0.92       107\n",
      "           3       0.94      0.98      0.96        97\n",
      "\n",
      "    accuracy                           0.93       399\n",
      "   macro avg       0.93      0.93      0.93       399\n",
      "weighted avg       0.93      0.93      0.93       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[100   6   0   0]\n",
      " [  5  81   3   0]\n",
      " [  0   5  96   6]\n",
      " [  0   0   2  95]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"======================================================\\nRandom Forest\")\n",
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# While the Random Forest model does show good results, the logistic regression is still\n",
    "# superior. The accuracy rises minimally by increasing the number of estimators, but\n",
    "# this comes at a heavy cost of computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Random Forest model does show good results, the logistic regression is still superior. The accuracy increases minimally by increasing the number of estimators, but this comes at a heavy cost of computational power, making the trade-off not worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are known for their versatility. They can be used for linearly separable data, as well as for non-linearly separable data. Since the logistic regression showed promising results, it is expected that the SVM will perform as well, if not better. This can be attributed to the margin the SVM adds, which helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "SVM\n",
      "Accuracy:  0.9624060150375939\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       106\n",
      "           1       0.94      0.98      0.96        89\n",
      "           2       0.96      0.93      0.94       107\n",
      "           3       0.96      0.97      0.96        97\n",
      "\n",
      "    accuracy                           0.96       399\n",
      "   macro avg       0.96      0.96      0.96       399\n",
      "weighted avg       0.96      0.96      0.96       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[104   2   0   0]\n",
      " [  1  87   1   0]\n",
      " [  0   4  99   4]\n",
      " [  0   0   3  94]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "print(\"======================================================\\nSVM\")\n",
    "svm = SVC(kernel='linear', C=2)\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# As expected, the SVM shows the best results so far, it even solves the problem\n",
    "# of showing low recall for the price_range of 2 faced by the logistic regression model.\n",
    "# The SVM model with a linear kernel shows great results, a poly kernel of degree 3 also\n",
    "# shows similar results, but the computational cost is a little higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the SVM shows the best results so far, it even solves the problem of showing low recall for the price_range of 2 faced by the logistic regression model. The SVM model with a linear kernel shows great results, a poly kernel of degree 3 also shows similar results, but the computational cost is a little higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "Decision Tree\n",
      "Accuracy:  0.8521303258145363\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90       106\n",
      "           1       0.78      0.80      0.79        89\n",
      "           2       0.80      0.84      0.82       107\n",
      "           3       0.90      0.88      0.89        97\n",
      "\n",
      "    accuracy                           0.85       399\n",
      "   macro avg       0.85      0.85      0.85       399\n",
      "weighted avg       0.85      0.85      0.85       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[94 12  0  0]\n",
      " [ 8 71 10  0]\n",
      " [ 0  8 90  9]\n",
      " [ 0  0 12 85]]\n",
      "======================================================\n",
      "======================================================\n",
      "Naive Bayes\n",
      "Accuracy:  0.8020050125313283\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.87      0.89       106\n",
      "           1       0.66      0.72      0.69        89\n",
      "           2       0.75      0.72      0.73       107\n",
      "           3       0.89      0.90      0.89        97\n",
      "\n",
      "    accuracy                           0.80       399\n",
      "   macro avg       0.80      0.80      0.80       399\n",
      "weighted avg       0.81      0.80      0.80       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[92 14  0  0]\n",
      " [ 9 64 16  0]\n",
      " [ 0 19 77 11]\n",
      " [ 0  0 10 87]]\n",
      "======================================================\n",
      "======================================================\n",
      "XGBoost\n",
      "Accuracy:  0.9373433583959899\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97       106\n",
      "           1       0.88      0.94      0.91        89\n",
      "           2       0.94      0.89      0.91       107\n",
      "           3       0.94      0.97      0.95        97\n",
      "\n",
      "    accuracy                           0.94       399\n",
      "   macro avg       0.94      0.94      0.94       399\n",
      "weighted avg       0.94      0.94      0.94       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[101   5   0   0]\n",
      " [  2  84   3   0]\n",
      " [  0   6  95   6]\n",
      " [  0   0   3  94]]\n",
      "======================================================\n",
      "======================================================\n",
      "Stacking\n",
      "Accuracy:  0.9624060150375939\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       106\n",
      "           1       0.96      0.97      0.96        89\n",
      "           2       0.95      0.93      0.94       107\n",
      "           3       0.95      0.97      0.96        97\n",
      "\n",
      "    accuracy                           0.96       399\n",
      "   macro avg       0.96      0.96      0.96       399\n",
      "weighted avg       0.96      0.96      0.96       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[104   2   0   0]\n",
      " [  1  86   2   0]\n",
      " [  0   2 100   5]\n",
      " [  0   0   3  94]]\n",
      "======================================================\n",
      "======================================================\n",
      "Voting\n",
      "Accuracy:  0.9448621553884712\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       106\n",
      "           1       0.90      0.93      0.92        89\n",
      "           2       0.95      0.92      0.93       107\n",
      "           3       0.95      0.98      0.96        97\n",
      "\n",
      "    accuracy                           0.94       399\n",
      "   macro avg       0.94      0.95      0.94       399\n",
      "weighted avg       0.95      0.94      0.94       399\n",
      "\n",
      "Confusion Matrix: \n",
      " [[101   5   0   0]\n",
      " [  3  83   3   0]\n",
      " [  0   4  98   5]\n",
      " [  0   0   2  95]]\n",
      "======================================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "print(\"======================================================\\nDecision Tree\")\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"======================================================\\nNaive Bayes\")\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "y_pred = nb.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# XGBoost\n",
    "print(\"======================================================\\nXGBoost\")\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "y_pred = xgb.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# Stacking\n",
    "print(\"======================================================\\nStacking\")\n",
    "estimators = [('rf', rf), ('dt', dt), ('knn', knn), ('svm', svm), ('lr', lr), ('nb', nb), ('xgb', xgb)]\n",
    "stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000))\n",
    "stack.fit(x_train, y_train)\n",
    "y_pred = stack.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n",
    "\n",
    "# Voting\n",
    "print(\"======================================================\\nVoting\")\n",
    "estimators = [('rf', rf), ('dt', dt), ('knn', knn), ('svm', svm), ('lr', lr), ('nb', nb), ('xgb', xgb)]\n",
    "vote = VotingClassifier(estimators=estimators)\n",
    "vote.fit(x_train, y_train)\n",
    "y_pred = vote.predict(x_val)\n",
    "ModelEvaluation(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models show results close to the SVM, but this is achieved through utilizing a number of different models, making the computational cost higher. In conclusion, the SVM model is the best trade-off between computational cost and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Outputs/svm_model.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the SVM model\n",
    "dump(svm, 'Outputs/svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "dataset = \"test\"\n",
    "test = pd.read_excel('Data/'+dataset+'.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading scaler to normalize the test data\n",
    "\n",
    "scaler = load('Outputs/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[:, 1:] = scaler.fit_transform(test.iloc[:, 1:])       # Normalizing the test data, but skipping the ID column\n",
    "x_test = test[sorted_correlations[1:5].index]                   # Choosing the top 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>...</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.362241</td>\n",
       "      <td>1</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118511</td>\n",
       "      <td>0.608550</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.227485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951613</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391190</td>\n",
       "      <td>0.237809</td>\n",
       "      <td>0.974772</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.871915</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665967</td>\n",
       "      <td>0.577822</td>\n",
       "      <td>0.572464</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.697799</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154693</td>\n",
       "      <td>0.835671</td>\n",
       "      <td>0.974235</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.623082</td>\n",
       "      <td>0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392764</td>\n",
       "      <td>0.206413</td>\n",
       "      <td>0.405260</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  battery_power  blue  clock_speed  dual_sim        fc  four_g   \n",
       "0   1       0.362241     1         0.52         1  0.736842       0  \\\n",
       "1   2       0.227485     1         0.00         1  0.210526       1   \n",
       "2   3       0.871915     1         0.92         0  0.052632       0   \n",
       "3   4       0.697799     0         0.00         1  0.947368       1   \n",
       "4   5       0.623082     0         0.36         0  0.578947       1   \n",
       "\n",
       "   int_memory     m_dep  mobile_wt  ...  px_height  px_width       ram   \n",
       "0    0.048387  0.000000   0.941667  ...   0.118511  0.608550  0.862319  \\\n",
       "1    0.951613  0.777778   0.925000  ...   0.391190  0.237809  0.974772   \n",
       "2    0.403226  0.888889   0.883333  ...   0.665967  0.577822  0.572464   \n",
       "3    0.370968  0.444444   0.133333  ...   0.154693  0.835671  0.974235   \n",
       "4    0.758065  0.444444   0.233333  ...   0.392764  0.206413  0.405260   \n",
       "\n",
       "       sc_h      sc_w  talk_time  three_g  touch_screen  wifi  price_range  \n",
       "0  0.500000  0.388889   0.000000        0             1     0            3  \n",
       "1  0.071429  0.000000   0.277778        1             0     0            3  \n",
       "2  0.857143  0.555556   0.444444        0             1     1            3  \n",
       "3  0.357143  0.000000   0.277778        1             1     0            3  \n",
       "4  0.714286  0.444444   0.277778        1             0     1            1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the test data\n",
    "y_pred = svm.predict(x_test)\n",
    "test['price_range'] = y_pred\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_range\n",
       "3    273\n",
       "0    260\n",
       "2    239\n",
       "1    228\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['price_range'].value_counts()     # Checking the distribution of the price ranges\n",
    "# It shows a balanced distribution, which is a good sign, as the data was itself balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The SVM models shows best results in minimum time. Predictions seem to be quite reasonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
